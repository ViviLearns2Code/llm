{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if torch.cuda.is_available():\n",
    "  with torch.cuda.device(device):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"./data/train.csv\")\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw, test_raw = train_test_split(data_raw, test_size=0.2)\n",
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an access token is needed if we use models like Mistral\n",
    "def get_access_token() -> str:\n",
    "    token = \"\"\n",
    "    with open('token.txt', 'r') as f:\n",
    "        token = f.read().strip()\n",
    "    print(token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_df, filename):\n",
    "    proc = pd.concat([raw_df['prompt'].apply(json.loads), raw_df['response_a'].apply(json.loads), raw_df['response_b'].apply(json.loads), raw_df['winner_model_a'].astype(np.float32), raw_df['winner_model_b'].astype(np.float32), raw_df['winner_tie'].astype(np.float32)], axis=1)\n",
    "    print(proc['prompt'].apply(len).max(), proc['prompt'].apply(len).min())\n",
    "    print(proc['response_a'].apply(len).max(), proc['response_a'].apply(len).min())\n",
    "    print(proc['response_b'].apply(len).max(), proc['response_b'].apply(len).min())\n",
    "    all(proc['response_b'].apply(len)==proc['response_a'].apply(len)) and all(proc['response_b'].apply(len)==proc['prompt'].apply(len))\n",
    "    final = proc.explode(['prompt', 'response_a', 'response_b'])\n",
    "    final.fillna(\"\",inplace=True)\n",
    "    print(final['response_a'].isna().sum(), final['response_b'].isna().sum())\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "    final['combined_a'] = final.apply(lambda x: x['prompt'] + tokenizer.sep_token + x['response_a'], axis=1)\n",
    "    final['combined_b'] = final.apply(lambda x: x['prompt'] + tokenizer.sep_token + x['response_b'], axis=1)\n",
    "    final.drop(['prompt', 'response_a', 'response_b'], axis=1, inplace=True)\n",
    "    final.to_pickle(filename)\n",
    "    final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(train_raw, \"./data/train_final.pkl\")\n",
    "preprocess(test_raw, \"./data/test_final.pkl\")\n",
    "del data_raw, train_raw, test_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', None)\n",
    "train = pd.read_pickle(\"./data/train_final.pkl\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_torch_dataloader(df, tokenizer, batch_size):\n",
    "    def encode(batch):\n",
    "        # tokenizer.__call__ will give ids\n",
    "        resultA = tokenizer(batch['combined_a'], return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "        resultB = tokenizer(batch['combined_b'], return_tensors=\"pt\", padding='max_length', truncation=True)\n",
    "        result = {\n",
    "            'input_ids_a': resultA[\"input_ids\"],\n",
    "            'attention_mask_a': resultA[\"attention_mask\"],\n",
    "            'input_ids_b': resultB[\"input_ids\"],\n",
    "            'attention_mask_b': resultB[\"attention_mask\"],\n",
    "            'winner_a': batch['winner_model_a'], \n",
    "            'winner_b': batch['winner_model_b'],\n",
    "            'tie': batch['winner_tie'], \n",
    "        }\n",
    "        return result\n",
    "\n",
    "    dataset = Dataset.from_pandas(df[['combined_a', 'combined_b', 'winner_model_a', 'winner_model_b', 'winner_tie']])\n",
    "    # tokenization is only applied when examples are accessed\n",
    "    dataset_tf = dataset.with_transform(encode)\n",
    "    dataset_tf.with_format(type='torch')\n",
    "    return torch.utils.data.DataLoader(dataset_tf, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "dl = create_torch_dataloader(train, tokenizer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO optimizations are possible by moving computation to GPU and using flash attention or reduce precision\n",
    "# TODO validation error\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # models are stored in /home/<user>/.cache/huggingface\n",
    "        # find /home/<user> -type d -name huggingface\n",
    "        # batch_size x sequence_length x embedding_size\n",
    "        self.llm = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "        # CLS token: batch_size x 1 x 768\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2*768, 3),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_a, attention_a, input_b, attention_b):\n",
    "        hidden_state_a = self.llm(input_a, attention_mask=attention_a).last_hidden_state\n",
    "        hidden_state_b = self.llm(input_b, attention_mask=attention_b).last_hidden_state\n",
    "        concat_state = torch.concat((hidden_state_a[:,0,:], hidden_state_b[:,0,:]), 1)\n",
    "        out = self.linear_relu_stack(concat_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/distilbert/distilbert-base-uncased/discussions/11\n",
    "# DistilBert with 66mio params should use 0,5GB + FC layer\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory consumption:\n",
    "# number of params in model: DistilBert with 66mio params should use 0,5GB + FC layer\n",
    "# x2 for autograd nodes\n",
    "# batch tensor: <batch_size>*512*768*8*4 (12MB per sample)\n",
    "# + 600MiB for bootstrapping GPU use for pytorch: https://stackoverflow.com/questions/62547072/why-does-pytorch-use-so-much-gpu-memory-to-store-tensors\n",
    "\n",
    "# still, it is unclear to me how GPU memory allocation works. When I run on Google Colab T4 GPU, a batch size of 10 already reaches the limit of 15GB GPU RAM...\n",
    "def training(epochs: int, dataloader, model):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    plot_pts = []\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        for j, batch in enumerate(dataloader):\n",
    "            # batch should use \n",
    "            batch['input_ids_a'].to(device)\n",
    "            batch['attention_mask_a'].to(device)\n",
    "            batch['input_ids_b'].to(device)\n",
    "            batch['attention_mask_b'].to(device)\n",
    "            for k, v in batch.items():\n",
    "                if k in ['input_ids_a', 'input_ids_b', 'attention_mask_a', 'attention_mask_b']:\n",
    "                  batch[k] = v.to(device)\n",
    "            target = torch.stack((batch[\"winner_a\"],batch[\"winner_b\"],batch[\"tie\"]),1).to(device)\n",
    "            output = model(batch['input_ids_a'],batch['attention_mask_a'],batch['input_ids_b'],batch['attention_mask_b'])\n",
    "            loss = loss_fn(output, target)\n",
    "            if j % 10 == 0:\n",
    "              print(f\"epoch {i} - batch {j} - loss: {loss}\")\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            plot_pts.append(loss.item())\n",
    "            loss.cpu()\n",
    "            del batch['input_ids_a'], batch['attention_mask_a'], batch['input_ids_b'], batch['attention_mask_b'],batch[\"winner_a\"],batch[\"winner_b\"],batch[\"tie\"]\n",
    "            del batch, target, output, loss\n",
    "            gc.collect()\n",
    "            if device == 'cpu':\n",
    "              continue\n",
    "            else:\n",
    "              with torch.cuda.device(device):\n",
    "                torch.cuda.empty_cache()\n",
    "    return plot_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts = training(1, dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_pts)\n",
    "plt.plot(plot_pts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
